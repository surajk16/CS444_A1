{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Colab Setup\n",
    "If you aren't using Colab, you can delete the following code cell. This is just to help students with mounting to Google Drive to access the other .py files and downloading the data, which is a little trickier on Colab than on your local machine using Jupyter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will be prompted with a window asking to grant permissions\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the path in your Google Drive in the string below. Note: do not escape slashes or spaces\n",
    "import os\n",
    "datadir = \"/content/assignment1\"\n",
    "if not os.path.exists(datadir):\n",
    "  !ln -s \"/content/drive/My Drive/YOUR PATH HERE/assignment1/\" $datadir\n",
    "os.chdir(datadir)\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading Fashion-MNIST\n",
    "import os\n",
    "os.chdir(os.path.join(datadir,\"fashion-mnist/\"))\n",
    "!chmod +x ./get_data.sh\n",
    "!./get_data.sh\n",
    "os.chdir(datadir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from data_process import get_FASHION_data, get_RICE_data\n",
    "from scipy.spatial import distance\n",
    "from models import Perceptron, SVM, Softmax, Logistic\n",
    "from kaggle_submission import output_submission_csv\n",
    "%matplotlib inline\n",
    "\n",
    "# For auto-reloading external modules\n",
    "# See http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Fashion-MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells we determine the number of images for each split and load the images.\n",
    "<br /> \n",
    "TRAIN_IMAGES + VAL_IMAGES = (0, 60000]\n",
    ", TEST_IMAGES = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change these numbers for experimentation\n",
    "# For submission we will use the default values \n",
    "TRAIN_IMAGES = 50000\n",
    "VAL_IMAGES = 10000\n",
    "normalize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_FASHION_data(TRAIN_IMAGES, VAL_IMAGES, normalize=normalize)\n",
    "X_train_fashion, y_train_fashion = data['X_train'], data['y_train']\n",
    "X_val_fashion, y_val_fashion = data['X_val'], data['y_val']\n",
    "X_test_fashion, y_test_fashion = data['X_test'], data['y_test']\n",
    "n_class_fashion = len(np.unique(y_test_fashion))\n",
    "n_dimension_fashion = X_train_fashion.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples:  10911\n",
      "Number of val samples:  3637\n",
      "Number of test samples:  3637\n"
     ]
    }
   ],
   "source": [
    "# loads train / test / val splits of 80%, 20%, 20% \n",
    "data = get_RICE_data()\n",
    "X_train_RICE, y_train_RICE = data['X_train'], data['y_train']\n",
    "X_val_RICE, y_val_RICE = data['X_val'], data['y_val']\n",
    "X_test_RICE, y_test_RICE = data['X_test'], data['y_test']\n",
    "n_class_RICE = len(np.unique(y_test_RICE))\n",
    "\n",
    "print(\"Number of train samples: \", X_train_RICE.shape[0])\n",
    "print(\"Number of val samples: \", X_val_RICE.shape[0])\n",
    "print(\"Number of test samples: \", X_test_RICE.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function computes how well your model performs using accuracy as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(pred, y_test):\n",
    "    return np.sum(y_test == pred) / len(y_test) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron has 2 hyperparameters that you can experiment with:\n",
    "- **Learning rate** - controls how much we change the current weights of the classifier during each update. We set it at a default value of 0.5, but you should experiment with different values. We recommend changing the learning rate by factors of 10 and observing how the performance of the classifier changes. You should also try adding a **decay** which slowly reduces the learning rate over each epoch.\n",
    "- **Number of Epochs** - An epoch is a complete iterative pass over all of the data in the dataset. During an epoch we predict a label using the classifier and then update the weights of the classifier according to the perceptron update rule for each sample in the training set. You should try different values for the number of training epochs and report your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will implement the Perceptron classifier in the **models/perceptron.py**\n",
    "\n",
    "The following code: \n",
    "- Creates an instance of the Perceptron classifier class \n",
    "- The train function of the Perceptron class is trained on the training data\n",
    "- We use the predict function to find the training accuracy as well as the testing accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Perceptron on Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5\n",
    "n_epochs = 10\n",
    "\n",
    "percept_fashion = Perceptron(n_class_fashion, lr, n_epochs)\n",
    "percept_fashion.train(X_train_fashion, y_train_fashion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_percept = percept_fashion.predict(X_train_fashion)\n",
    "print('The training accuracy is given by: %f' % (get_acc(pred_percept, y_train_fashion)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Perceptron on Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_percept = percept_fashion.predict(X_val_fashion)\n",
    "print('The validation accuracy is given by: %f' % (get_acc(pred_percept, y_val_fashion)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Perceptron on Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_percept = percept_fashion.predict(X_test_fashion)\n",
    "print('The testing accuracy is given by: %f' % (get_acc(pred_percept, y_test_fashion)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron_Fashion-MNIST Kaggle Submission\n",
    "\n",
    "Once you are satisfied with your solution and test accuracy, output a file to submit your test set predictions to the Kaggle for Assignment 1 Fashion-MNIST. Use the following code to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_submission_csv('kaggle/perceptron_submission_fashion.csv', percept_fashion.predict(X_test_fashion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Perceptron on Rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5\n",
    "n_epochs = 10\n",
    "\n",
    "percept_RICE = Perceptron(n_class_RICE, lr, n_epochs)\n",
    "percept_RICE.train(X_train_RICE, y_train_RICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_percept = percept_RICE.predict(X_train_RICE)\n",
    "print('The training accuracy is given by: %f' % (get_acc(pred_percept, y_train_RICE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Perceptron on Rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_percept = percept_RICE.predict(X_val_RICE)\n",
    "print('The validation accuracy is given by: %f' % (get_acc(pred_percept, y_val_RICE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Perceptron on Rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_percept = percept_RICE.predict(X_test_RICE)\n",
    "print('The testing accuracy is given by: %f' % (get_acc(pred_percept, y_test_RICE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (with SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will implement a \"soft margin\" SVM. In this formulation you will maximize the margin between positive and negative training examples and penalize margin violations using a hinge loss.\n",
    "\n",
    "We will optimize the SVM loss using SGD. This means you must compute the loss function with respect to model weights. You will use this gradient to update the model weights.\n",
    "\n",
    "SVM optimized with SGD has 3 hyperparameters that you can experiment with:\n",
    "- **Learning rate** - similar to as defined above in Perceptron, this parameter scales by how much the weights are changed according to the calculated gradient update. \n",
    "- **Epochs** - similar to as defined above in Perceptron.\n",
    "- **Regularization constant** - Hyperparameter to determine the strength of regularization. In this case it is a coefficient on the term which maximizes the margin. You could try different values. The default value is set to 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will implement the SVM using SGD in the **models/svm.py**\n",
    "\n",
    "The following code: \n",
    "- Creates an instance of the SVM classifier class \n",
    "- The train function of the SVM class is trained on the training data\n",
    "- We use the predict function to find the training accuracy as well as the testing accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SVM on Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SVM.__init__() takes 5 positional arguments but 7 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m reg_const \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m\n\u001b[0;32m      4\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m----> 6\u001b[0m svm_fashion \u001b[38;5;241m=\u001b[39m \u001b[43mSVM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_class_fashion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_dimension_fashion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg_const\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m svm_fashion\u001b[38;5;241m.\u001b[39mtrain(X_train_fashion, y_train_fashion)\n",
      "\u001b[1;31mTypeError\u001b[0m: SVM.__init__() takes 5 positional arguments but 7 were given"
     ]
    }
   ],
   "source": [
    "lr = 0.5\n",
    "n_epochs = 10\n",
    "reg_const = 0.05\n",
    "\n",
    "svm_fashion = SVM(n_class_fashion, lr, n_epochs, reg_const)\n",
    "svm_fashion.train(X_train_fashion, y_train_fashion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svm = svm_fashion.predict(X_train_fashion)\n",
    "print('The training accuracy is given by: %f' % (get_acc(pred_svm, y_train_fashion)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate SVM on Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svm = svm_fashion.predict(X_val_fashion)\n",
    "print('The validation accuracy is given by: %f' % (get_acc(pred_svm, y_val_fashion)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test SVM on Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svm = svm_fashion.predict(X_test_fashion)\n",
    "print('The testing accuracy is given by: %f' % (get_acc(pred_svm, y_test_fashion)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM_Fashion-MNIST Kaggle Submission\n",
    "\n",
    "Once you are satisfied with your solution and test accuracy output a file to submit your test set predictions to the Kaggle for Assignment 1 Fashion-MNIST. Use the following code to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_submission_csv('kaggle/svm_submission_fashion.csv', svm_fashion.predict(X_test_fashion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SVM on Rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5\n",
    "n_epochs = 10\n",
    "reg_const = 0.05\n",
    "\n",
    "svm_RICE = SVM(n_class_RICE, lr, n_epochs, reg_const)\n",
    "svm_RICE.train(X_train_RICE, y_train_RICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svm = svm_RICE.predict(X_train_RICE)\n",
    "print('The training accuracy is given by: %f' % (get_acc(pred_svm, y_train_RICE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate SVM on Rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svm = svm_RICE.predict(X_val_RICE)\n",
    "print('The validation accuracy is given by: %f' % (get_acc(pred_svm, y_val_RICE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test SVM on Rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svm = svm_RICE.predict(X_test_RICE)\n",
    "print('The testing accuracy is given by: %f' % (get_acc(pred_svm, y_test_RICE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classifier (with SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next, you will train a Softmax classifier. This classifier consists of a linear function of the input data followed by a softmax function which outputs a vector of dimension C (number of classes) for each data point. Each entry of the softmax output vector corresponds to a confidence in one of the C classes, and like a probability distribution, the entries of the output vector sum to 1. We use a cross-entropy loss on this sotmax output to train the model. \n",
    "\n",
    "Check the following link as an additional resource on softmax classification: http://cs231n.github.io/linear-classify/#softmax\n",
    "\n",
    "Once again we will train the classifier with SGD. This means you need to compute the gradients of the softmax cross-entropy loss function according to the weights and update the weights using this gradient. Check the following link to help with implementing the gradient updates: https://deepnotes.io/softmax-crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax classifier has 3 hyperparameters that you can experiment with:\n",
    "- **Learning rate** - As above, this controls how much the model weights are updated with respect to their gradient.\n",
    "- **Number of Epochs** - As described for perceptron.\n",
    "- **Regularization constant** - Hyperparameter to determine the strength of regularization. In this case, we minimize the L2 norm of the model weights as regularization, so the regularization constant is a coefficient on the L2 norm in the combined cross-entropy and regularization objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will implement a softmax classifier using SGD in the **models/softmax.py**\n",
    "\n",
    "The following code: \n",
    "- Creates an instance of the Softmax classifier class \n",
    "- The train function of the Softmax class is trained on the training data\n",
    "- We use the predict function to find the training accuracy as well as the testing accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Softmax on Fashion-MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate: 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500 loss: 2.3036741067546602\n",
      "Epoch 101/500 loss: 0.5169439184343881\n",
      "Epoch 201/500 loss: 0.4930173173365047\n",
      "Epoch 301/500 loss: 0.45343070561707427\n",
      "Epoch 401/500 loss: 0.47765172101558134\n"
     ]
    }
   ],
   "source": [
    "lr = 0.5\n",
    "n_epochs = 500\n",
    "reg_const = 0.1\n",
    "batch_size = 512\n",
    "\n",
    "softmax_fashion = Softmax(n_class_fashion, lr, batch_size, n_epochs, reg_const)\n",
    "softmax_fashion.train(X_train_fashion, y_train_fashion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,) (50000,)\n",
      "The training accuracy is given by: 84.342000\n"
     ]
    }
   ],
   "source": [
    "pred_softmax = softmax_fashion.predict(X_train_fashion)\n",
    "print('The training accuracy is given by: %f' % (get_acc(pred_softmax, y_train_fashion)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Softmax on Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation accuracy is given by: 83.180000\n"
     ]
    }
   ],
   "source": [
    "pred_softmax = softmax_fashion.predict(X_val_fashion)\n",
    "print('The validation accuracy is given by: %f' % (get_acc(pred_softmax, y_val_fashion)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Softmax on Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing accuracy is given by: 82.440000\n"
     ]
    }
   ],
   "source": [
    "pred_softmax = softmax_fashion.predict(X_test_fashion)\n",
    "print('The testing accuracy is given by: %f' % (get_acc(pred_softmax, y_test_fashion)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax_Fashion-MNIST Kaggle Submission\n",
    "\n",
    "Once you are satisfied with your solution and test accuracy output a file to submit your test set predictions to the Kaggle for Assignment 1 Fashion-MNIST. Use the following code to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_submission_csv('kaggle/softmax_submission_fashion.csv', softmax_fashion.predict(X_test_fashion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Softmax on Rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 loss: 0.6814525555249616\n",
      "Epoch 101/1000 loss: 0.3323327177436419\n",
      "Epoch 201/1000 loss: 0.3361037245976321\n",
      "Epoch 301/1000 loss: 0.3441710055904028\n",
      "Epoch 401/1000 loss: 0.3322991944393018\n",
      "Epoch 501/1000 loss: 0.34037130027773954\n",
      "Epoch 601/1000 loss: 0.34939797692219726\n",
      "Epoch 701/1000 loss: 0.35118908255390296\n",
      "Epoch 801/1000 loss: 0.3378305837622793\n",
      "Epoch 901/1000 loss: 0.34837510563493346\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0005\n",
    "n_epochs = 1000\n",
    "reg_const = 0.1\n",
    "batch_size = 1024\n",
    "\n",
    "softmax_RICE = Softmax(n_class_RICE, lr, batch_size, n_epochs, reg_const)\n",
    "softmax_RICE.train(X_train_RICE, y_train_RICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is given by: 53.120704\n"
     ]
    }
   ],
   "source": [
    "pred_softmax = softmax_RICE.predict(X_train_RICE)\n",
    "print('The training accuracy is given by: %f' % (get_acc(pred_softmax, y_train_RICE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Softmax on Rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation accuracy is given by: 64.118779\n"
     ]
    }
   ],
   "source": [
    "pred_softmax = softmax_RICE.predict(X_val_RICE)\n",
    "print('The validation accuracy is given by: %f' % (get_acc(pred_softmax, y_val_RICE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Softmax on Rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3637,) (3637,)\n",
      "# correct:  [1 1 1 ... 1 1 1]\n",
      "# total:  3637\n",
      "The testing accuracy is given by: 62.359087\n"
     ]
    }
   ],
   "source": [
    "pred_softmax = softmax_RICE.predict(X_test_RICE)\n",
    "print('The testing accuracy is given by: %f' % (get_acc(pred_softmax, y_test_RICE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Classifier has 2 hyperparameters that you can experiment with:\n",
    "- **Learning rate** - similar to as defined above in Perceptron, this parameter scales by how much the weights are changed according to the calculated gradient update. \n",
    "- **Number of Epochs** - As described for perceptron.\n",
    "- **Threshold** - The decision boundary of the classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will implement the Logistic Classifier in the **models/logistic.py**\n",
    "\n",
    "The following code: \n",
    "- Creates an instance of the Logistic classifier class \n",
    "- The train function of the Logistic class is trained on the training data\n",
    "- We use the predict function to find the training accuracy as well as the testing accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Logistic Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 12)\n",
      "Epoch 1/1000 loss: 0.7044360842848099\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 11/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 21/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 31/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 41/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 51/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 61/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 71/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 81/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 91/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 101/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 111/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 121/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 131/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 141/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 151/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 161/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 171/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 181/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 191/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 201/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 211/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 221/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 231/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 241/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 251/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 261/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 271/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 281/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 291/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 301/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 311/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 321/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 331/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 341/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 351/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 361/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 371/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 381/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 391/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 401/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 411/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 421/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 431/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 441/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 451/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 461/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 471/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 481/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 491/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 501/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 511/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 521/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 531/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 541/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 551/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 561/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 571/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 581/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 591/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 601/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 611/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 621/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 631/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 641/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 651/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 661/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 671/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 681/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 691/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 701/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 711/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 721/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 731/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 741/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 751/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 761/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 771/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 781/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 791/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 801/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 811/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 821/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 831/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 841/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 851/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\UIUC\\Spring 2023\\CS444 - Deep Learning for Computer Vision\\assignment1\\models\\logistic.py:33: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "D:\\UIUC\\Spring 2023\\CS444 - Deep Learning for Computer Vision\\assignment1\\models\\logistic.py:73: RuntimeWarning: divide by zero encountered in log\n",
      "  data_loss = -np.mean(np.log(self.sigmoid(y_train * output)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 861/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 871/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 881/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 891/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 901/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 911/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 921/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 931/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 941/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 951/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 961/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 971/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 981/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "Epoch 991/1000 loss: inf\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n",
      "(1, 12)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "n_epochs = 100\n",
    "threshold = 0.5\n",
    "batch_size = 256\n",
    "reg_const = 0.1\n",
    "\n",
    "lr = Logistic(learning_rate, n_epochs, threshold, batch_size, reg_const)\n",
    "lr.train(X_train_RICE, y_train_RICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.] [0 1]\n",
      "The training accuracy is given by: 45.220420\n"
     ]
    }
   ],
   "source": [
    "pred_lr = lr.predict(X_train_RICE)\n",
    "print(np.unique(pred_lr), np.unique(y_train_RICE))\n",
    "print('The training accuracy is given by: %f' % (get_acc(pred_lr, y_train_RICE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Logistic Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation accuracy is given by: 43.854825\n"
     ]
    }
   ],
   "source": [
    "pred_lr = lr.predict(X_val_RICE)\n",
    "print('The validation accuracy is given by: %f' % (get_acc(pred_lr, y_val_RICE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Logistic Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing accuracy is given by: 45.944460\n"
     ]
    }
   ],
   "source": [
    "pred_lr = lr.predict(X_test_RICE)\n",
    "print('The testing accuracy is given by: %f' % (get_acc(pred_lr, y_test_RICE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
